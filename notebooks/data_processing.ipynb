{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dylan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Set, Union\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from PyDictionary import PyDictionary\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Basic Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_basic_word_list(stopwords: Union[List[str], Set[str]] = nltk.corpus.stopwords.words('english')) -> Set[str]:\n",
    "    \"\"\"Loads the Ogden Basic English Word List and merges it with the Wikipedia Basic Word List to create a set\n",
    "    of basic English words, filtering out all stopwords.\n",
    "    \n",
    "    Requires the files ogden_basic_words.txt and wikipedia_words.txt stored in the same directory under the subdirectory \n",
    "    /data (e.g. current_directory/data/ogden_basic_words.txt).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    stopwords: Union[List[str], Set[str]]\n",
    "        Words to filter out of the basic word list\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Set[str]\n",
    "        All basic English words\n",
    "    \"\"\"\n",
    "    stopwords = set(stopwords)  # Ensure set to leverage hashing\n",
    "    \n",
    "    def load_ogden_basic_words(path: str) -> Set[str]:\n",
    "        with open(path, 'r') as file:\n",
    "            raw_words = file.read().replace('\\n', ',').lower().split(',')\n",
    "            \n",
    "        words = []\n",
    "        for raw_word in raw_words:\n",
    "            word = raw_word\n",
    "            for value in ['basic:', 'intl:', 'endings:', 'compound:', 'next:', '(', ')', '.']:\n",
    "                word = word.replace(value, '')\n",
    "            words.append(word.strip())\n",
    "        return set(word for word in words if word)\n",
    "            \n",
    "    def load_wikipedia_words(path: str) -> Set[str]:\n",
    "        with open(path, 'r') as file:\n",
    "            raw_words = file.read().replace('\\n', '').replace('.', '').lower().split(',')\n",
    "        return set(word.strip() for word in raw_words if word)\n",
    "    \n",
    "    basic_word_list: set = set([])\n",
    "    word_list_directory = Path().resolve().parents[0] / 'data' / 'basic_word_lists'\n",
    "    basic_word_lists: Dict[str, str] = {\n",
    "        'ogden_basic_words.txt': load_ogden_basic_words,\n",
    "        'wikipedia_words.txt': load_wikipedia_words\n",
    "    }\n",
    "    for filename, read_function in basic_word_lists.items():\n",
    "        basic_word_list |= read_function(word_list_directory / filename)\n",
    "    \n",
    "    return set(word for word in basic_word_list if word not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_word_list = load_basic_word_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token and Tokenizer Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    \n",
    "    def __init__(self, token: str):\n",
    "        self.string: str = token.strip()\n",
    "        self.dictionary: PyDictionary = PyDictionary(self.string)\n",
    "        self.complexity_score: int = 0\n",
    "            \n",
    "    def initialize_all_attributes(self, basic_word_list: Union[list, set]) -> bool:\n",
    "        \"\"\"Initializes all class attributes to avoid manually running down the list each time.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        basic_word_list: Union[List[str], Set[str]]\n",
    "            the set of \"Basic\" words for self.semantic_word_complexity\n",
    "    \n",
    "        Returns\n",
    "        ----------\n",
    "        bool\n",
    "            successfully initialized everything?\n",
    "        \"\"\"\n",
    "        self.get_semantic_word_complexity(basic_word_list)\n",
    "        return True\n",
    "        \n",
    "    def get_semantic_word_complexity(self, basic_word_list: Union[List[str], Set[str]]) -> int:\n",
    "        \"\"\"Follows the algorithm proposed by the research paper \"Estimation of Lexical Complexity using Language Semantics\"\n",
    "        by Nimavat and Champaneria. https://www.researchgate.net/publication/320466030\n",
    "        \n",
    "        :param Union[List[str], Set[str]] basic_word_list: the set of \"Basic\" words according to the proposed algorithm\n",
    "        :return int: the Semantic Word Complexity Score\n",
    "        \"\"\"\n",
    "        if self.complexity_score:\n",
    "            return self.complexity_score\n",
    "        \n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        tokens = self.get_definition_as_tokens(self.string, stopwords)\n",
    "        for token in tokens:\n",
    "            if token in basic_word_list:\n",
    "                self.complexity_score += 1\n",
    "            else:\n",
    "                tokens2 = self.get_definition_as_tokens(token, stopwords)\n",
    "                self.complexity_score += len(tokens2)\n",
    "        return self.complexity_score\n",
    "            \n",
    "    def get_definition_as_tokens(self, word: str, stopwords: Union[List[str], Set[str]] = []) -> List[str]:\n",
    "        \"\"\"Converts the first definition for the provided word from PyDictionary, filtering our the provided stopwords.\n",
    "        \n",
    "        :param str word: the word to look up the definition of\n",
    "        :param Union[list, set] stopwords: the words to remove from the definition \n",
    "        :return List[str]: list of all alphabetical, non-stopword tokens lowercase\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        if definitions := self.dictionary.meaning(word):\n",
    "            for pos, pos_definitions in definitions.items():\n",
    "                for definition in (pos_definitions or []):\n",
    "                    raw_tokens = nltk.word_tokenize(re.sub(r'\\((.*?)$', '', definition))\n",
    "                    tokens.extend([t.lower() for t in raw_tokens if t.lower() not in stopwords and t.isalpha()])\n",
    "                    if tokens:\n",
    "                        return tokens\n",
    "        return tokens\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Converts all basic-type attributes from this class into a nested dictionary following the pattern:\n",
    "        {'self.string': {'complexity_score': 'self.complexity_score', ...}}\n",
    "        \n",
    "        :return Dict[str, Any]: dictionary with all class attribute names as keys and values and values\n",
    "        \"\"\"\n",
    "        allowed_types = (str, int, float, list, dict, set, tuple)\n",
    "        as_dict = {key: value for key, value in vars(self).items() if isinstance(value, allowed_types)}\n",
    "        return {as_dict.pop('string'): as_dict}\n",
    "    \n",
    "    def from_dict(self, as_dict: Dict[str, Dict[str, Any]]) -> bool:\n",
    "        \"\"\"Converts a dictionary into this class's attributes to prevent recalculate metrics every time.\n",
    "        as_dict should follow the following pattern:\n",
    "        {'self.string': {'complexity_score': 'self.complexity_score', ...}}\n",
    "        \n",
    "        :return bool: successfully initialized self attributes?\n",
    "        \"\"\"\n",
    "        class_attributes = vars(self)\n",
    "        for string, attributes in as_dict.items():\n",
    "            if self.string != string.strip():\n",
    "                continue\n",
    "            for attribute_name, attribute_value in attributes.items():\n",
    "                if attribute_name in class_attributes:\n",
    "                    self.__setattr__(attribute_name, attribute_value)\n",
    "            break\n",
    "        else:\n",
    "            raise Exception(f\"The passed dictionary (as_dict) has no key in it matching self.string: {self.string}\")\n",
    "        return True\n",
    "    \n",
    "\n",
    "class TokenFeaturizer:\n",
    "    \n",
    "    def __init__(self, path: Union[str, None] = None):\n",
    "        self.path: Union[str, Path] = path or Path().resolve() / 'all_tokens.json'\n",
    "        self.all_tokens: Dict[str, Dict[str, str]] = self.load_all_tokens()\n",
    "        self.average_complexity_score: float = 0.0\n",
    "        self.basic_word_list: Union[List[str], Set[str]] = load_basic_word_list()\n",
    "        \n",
    "    def get_token_object(self, word: str) -> Token:\n",
    "        \"\"\"If the word is already in self.all_tokens data, will initialize a new Token object using this data and the\n",
    "        Token.from_dict() method. Otherwise, create a new Token instance, initialize its attributes, and store it in\n",
    "        self.all_tokens to avoid reinitialization in the future.\n",
    "        \n",
    "        :param str word: the word to create a Token from\n",
    "        :return Token: an instance of the Token class initialized on the parameter word\n",
    "        \"\"\"\n",
    "        token = Token(word)\n",
    "        if token_data := self.all_tokens.get(word):  # Word already in all_tokens\n",
    "            if all([key in token_data for key in token.to_dict().get(word, {})]):  # No new attributes in Token class\n",
    "                processed_word = word.strip()\n",
    "                token.from_dict({processed_word: token_data})\n",
    "                return token\n",
    "            \n",
    "        token.initialize_all_attributes(self.basic_word_list)\n",
    "        self.all_tokens.update(token.to_dict())\n",
    "        return token\n",
    "    \n",
    "    def get_average_complexity_score(self) -> float:\n",
    "        \"\"\"Averages the complexity score of all non-zero complexity tokens in self.all_tokens.\n",
    "        \n",
    "        :return float: the average complexity score (excluding tokens with complexity score of zero)\n",
    "        \"\"\"\n",
    "        if not self.average_complexity_score:\n",
    "            non_zero_tokens = 0\n",
    "            total_complexity_score = 0\n",
    "            for token, token_data in self.all_tokens.items():\n",
    "                if complexity_score := token_data.get('complexity_score'):\n",
    "                    non_zero_tokens += 1\n",
    "                    total_complexity_score += complexity_score\n",
    "            self.average_complexity_score = np.round(total_complexity_score / non_zero_tokens, 2)\n",
    "        return self.average_complexity_score\n",
    "            \n",
    "    def load_all_tokens(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Given the proper path to the all_tokens.json file, convert the file into a dictionary for faster look up.\n",
    "        This dictionary prevents you from recalculating the attributes of all known tokens.\n",
    "\n",
    "        :return Dict[str, Dict[str, Any]]: token name as a key with its associated metrics \n",
    "        \"\"\"\n",
    "        if not self.path.is_file():\n",
    "            return {}\n",
    "        with open(self.path, 'r') as file:\n",
    "            return json.load(file)\n",
    "        \n",
    "    def save_all_tokens(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Given the proper path to the all_tokens.json file, convert the file into a dictionary for faster look up.\n",
    "        This dictionary prevents you from recalculating the attributes of all known tokens.\n",
    "\n",
    "        :return Dict[str, Dict[str, Any]]: token name as a key with its associated metrics \n",
    "        \"\"\"\n",
    "        with open(self.path, 'w') as file:\n",
    "            json.dump(self.all_tokens, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_json_path = Path().resolve().parents[0] / 'data' / 'all_tokens.json'\n",
    "tf = TokenFeaturizer(all_tokens_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Training DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_lrb_rrb_tags(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Removes -LRB-...-RRB- tags along with all text between from 'original_text' column.\n",
    "    \n",
    "    Expected dataframe columns:\n",
    "        original_text: str\n",
    "    \n",
    "    :return pd.DataFrame: the passed DataFrame cleaned\n",
    "    \"\"\"\n",
    "    dataframe.original_text = dataframe.original_text.str.replace(r' -LRB-(.*?)-RRB-', '', regex=True)\n",
    "    return dataframe\n",
    "\n",
    "def replace_html_char_codes_with_char(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Replaces char codes (e.g. &ndash;) with their respective characters.\n",
    "    \n",
    "    Expected dataframe columns:\n",
    "        original_text: str\n",
    "    \n",
    "    :return pd.DataFrame: the passed DataFrame cleaned\n",
    "    \"\"\" # TODO: Find/compile CSV of all codes and dynamically import and replace all\n",
    "    characters: Dict[str, str] = { # Code: Character\n",
    "        '&ndash;': '–',\n",
    "        '&mdash;': '—',\n",
    "    }\n",
    "    for code, character in characters.items():\n",
    "        dataframe.original_text = dataframe.original_text.str.replace(f\"{code[0]} {code[1:-1]} {code[-1]}\", character)\n",
    "    return dataframe\n",
    "\n",
    "def clean_dataset(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Performs all required data cleaning on all records in the passed DataFrame.\n",
    "    \n",
    "    Expected dataframe columns:\n",
    "        original_text: str\n",
    "        label: int (0 or 1)\n",
    "    \n",
    "    :return pd.DataFrame: the passed DataFrame cleaned\n",
    "    \"\"\"\n",
    "    cleaning_steps = [\n",
    "        remove_lrb_rrb_tags,\n",
    "        replace_html_char_codes_with_char,\n",
    "    ]\n",
    "    for cleaning in cleaning_steps:\n",
    "        dataframe = cleaning(dataframe)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = Path().resolve().parents[0] / 'data' / 'WikiLarge_Train.csv'\n",
    "df = pd.read_csv(training_path)\n",
    "df = clean_dataset(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_path = Path().resolve().parents[0] / 'data' / 'WikiLarge_Test.csv'\n",
    "df2 = pd.read_csv(testing_path)\n",
    "df2 = clean_dataset(df2)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize All Complexity Score Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentence(sentence: str, \n",
    "                           stopwords: Union[List[str], Set[str]] = nltk.corpus.stopwords.words('english')) -> List[str]:\n",
    "    \"\"\"Convert the sentence str into a list of tokens, filtering our the provided stopwords.\n",
    "        \n",
    "    :param str sentence: the sentence string to convert\n",
    "    :param Union[List[str], Set[str]] stopwords: the words to remove from the definition \n",
    "    :return List[str]: list of all alphabetical, non-stopword tokens lowercase\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    return [token.lower() for token in tokens if token.lower() not in stopwords and token.isalpha()]\n",
    "\n",
    "def get_complexity_scores(tf: TokenFeaturizer, sentence: str) -> List[Union[int, float]]:\n",
    "    \"\"\"Calculates a list of complexity scores for all valid tokens in sentence. Will replace all 0 complexity scores with\n",
    "    the average complexity score of all_tokens stored in the param tf.\n",
    "    \n",
    "    :param TokenFeaturizer tf: an initialized TokenFeaturizer instance\n",
    "    :param str sentence: the sentence to calculate the complexity score of\n",
    "    :return List[Union[int, float]]: all complexity scores for non-stopword tokens in sentence\n",
    "    \"\"\"\n",
    "    return [cs if (cs := tf.get_token_object(tok).complexity_score) else tf.get_average_complexity_score()\n",
    "            for tok in get_tokenized_sentence(sentence)]\n",
    "\n",
    "def process_features(df: pd.DataFrame, all_tokens_path: Union[Path, str]) -> pd.DataFrame:\n",
    "    \"\"\"Adds the following columns to the parameter DataFrame: [\n",
    "        'text_length': int,  # length of all tokens in original_text\n",
    "        'token_count': int,  # all non-stopword tokens\n",
    "        'stopword_count': int,\n",
    "        'complexity_scores': List[float],  # complexity scores of all non-stopword tokens (zeros replaced by average)\n",
    "        'sum_cs': float,\n",
    "        'avg_cs': float,\n",
    "    ]\n",
    "    \n",
    "    :param pd.DataFrame df: required column: 'original_text': str\n",
    "    :return pd.DataFrame: the original dataframe with more feature columns\n",
    "    \"\"\"\n",
    "    tf = TokenFeaturizer(all_tokens_path)\n",
    "    df['text_length'] = df['original_text'].str.count(' ') + 1\n",
    "    df['complexity_scores'] = df['original_text'].apply(lambda x: get_complexity_scores(tf, x))\n",
    "    df['token_count'] = df['complexity_scores'].str.len()\n",
    "    df['stopword_count'] = df['text_length'] - df['token_count']\n",
    "    df['sum_cs'] = df['complexity_scores'].map(sum)\n",
    "    df['avg_cs'] = df['sum_cs'] / df['token_count']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_features(df, all_tokens_json_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
